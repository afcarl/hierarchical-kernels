\documentclass{article}
\renewcommand{\subparagraph}{\paragraph}
\usepackage{include/nips13submit_e}
%\usepackage{theapa}
\usepackage{times}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{pstricks}
\usepackage{pst-tree}
%\usepackage{color}
%\usepackage{makeidx}  % allows for indexgeneration
\usepackage{bm}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{array}
\usepackage{colortbl}
\usepackage{framed}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage{sgame}
\usepackage{dsfont}
\def\sgtextcolor{black}
\def\sglinecolor{black}
%\renewcommand{\gamestretch}{2}
\usepackage{multicol}
\usepackage{lscape}
\usepackage{relsize}
\usepackage{rotating}
\usepackage{tikz}
\usetikzlibrary{calc}

% ============== Mike's commands ==============
\usepackage{nicefrac}
\newcommand{\vect}[1]{\underline{\smash{#1}}}
\renewcommand{\v}[1]{\vect{#1}}
\newcommand{\reals}{\mathds{R}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\br}{^{\text{\textnormal{ r}}}}
\newcommand{\cat}{^{\text{\textnormal{c}}}}
% ============== ==============

\newcommand{\cut}[1]{}
\newcommand{\hide}[1]{}
\renewcommand{\blue}[1]{{\textcolor{blue}{#1}}}
%\renewcommand{\blue}[1]{#1}

%% Zapf Chancery font: has lowercase script letters
\DeclareFontFamily{OT1}{pzc}{}
\DeclareFontShape{OT1}{pzc}{m}{it}{<-> s * [1.200] pzcmi7t}{}
\DeclareMathAlphabet{\mathscr}{OT1}{pzc}{m}{it}

\newcommand\transpose{{\textrm{\tiny{\sf{T}}}}}
\newcommand{\note}[1]{}
\newcommand{\hlinespace}{~\vspace*{-0.15cm}~\\\hline\\\vspace*{0.15cm}}
%\newcommand{\hlinespace}{~\vspace*{0.45cm}\\\hline\\~\vspace*{-0.9cm}}
%\newcommand{\hlinespace}{~\vspace*{0.05cm}\\\hline~\vspace*{0.5cm}}

% comment the next line to turn off notes
\renewcommand{\note}[1]{~\\\frame{\begin{minipage}[c]{\textwidth}\vspace{2pt}\center{#1}\vspace{2pt}\end{minipage}}\vspace{3pt}\\}
\newcommand{\lnote}[1]{\note{#1}}
\newcommand{\emcite}[1]{\citet{#1}}
\newcommand{\yrcite}[1]{\citeyear{#1}}
\newcommand{\aunpcite}[1]{\citeR{#1}}

\newcommand{\heavyrule}{\specialrule{\heavyrulewidth}{.4em}{.4em}}
\newcommand{\lightrule}{\specialrule{.03em}{.4em}{.4em}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% keep figures from going onto a page by themselves
\renewcommand{\topfraction}{0.9}
\renewcommand{\textfraction}{0.07}
\renewcommand{\floatpagefraction}{0.9}
\renewcommand{\dbltopfraction}{0.9}      % for double-column styles
\renewcommand{\dblfloatpagefraction}{0.7}   % for double-column styles



\usepackage{amsmath, amsthm, amssymb}
\newtheorem{thm}{Theorem}%[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}[thm]{Observation}

%\theoremstyle{definition}
\newtheorem{define}[thm]{Definition}
\hyphenation{ge-ne-ral-ize}




\newcommand{\Var}{\ensuremath\text{Var}}
\newcommand{\indicator}{\ensuremath\mathds{I}}

\newcommand{\fhspace}{\vspace*{0.2cm}}
\newcommand{\newsec}{\hspace{0cm}}

% replaces tabular; takes same arguments. use \midrule for a rule, no vertical rules, and eg \cmidrule(l){2-3} as needed with \multicolumn
\newenvironment{ktabular}[1]{\sffamily\small\begin{center}\begin{tabular}[c]{#1}\toprule}{\bottomrule \end{tabular}\end{center}\normalsize\rmfamily\vspace{-5pt}}
\newcommand{\tbold}[1]{\textbf{#1}}
\newcommand{\interrowspace}{.6em}

\nipsfinalcopy

\begin{document}

\title{Raiders of the Lost Architecture:\\Kernels for Conditional Parameter Spaces}

\author{
Kevin Swersky \\
University of Toronto \\
\texttt{kswesrky@cs.utoronto.edu} \\
\And
David Duvenaud \\
University of Cambridge \\
\texttt{dkd23@cam.ac.uk} \\
\And
Jasper Snoek\\
Harvard University \\
\texttt{jsnoek@seas.harvard.edu} \\
\AND
Frank Hutter  \\
Freiburg University \\
{\tt fh@informatik.uni-freiburg.de} \\
\And
Michael A. Osborne \\
University of Oxford \\
{\tt mosb@robots.ox.ac.uk} \\
}




\maketitle
\begin{abstract}
When performing model-based optimization, we must often search over structures with differing numbers of parameters.  For instance, we may wish to search over neural network architectures with an unkown number of layers.  To combine information between different architectures, we define a family of kernels for conditional parameter spaces.
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recently, Bayesian optimization has been used to learn parameters for neural networks \cite{snoek-etal-2012b}.
Because different neural net architectures have different numbers of parameters (or their parameters have different meanings), it is not clear how to combine information or extrapolate between the performance of different architectures.
Historically, practitioners have simply built a separate model for each type of architecture \cite{bergstra2011algorithms}.  However, if there is any relation between networks with different architectures, seperately modeling each architecture is wasteful.

Bayesian optimization methods rely on constructing a model of the function being optimized.  We model this function using Gaussian process priors \cite{rasmussen38gaussian}, where model assumptions are defined through the kernel.  In this paper, we introduce a simple method for building a Gaussian process model over parameter spaces with varying numbers of parameters.  We do this by defining an embedding of datapoints depending on which parameters are active, then perform standard regression in this new space.





\section{A Kernel for Conditional Parameter Spaces}

In this section, we construct a kernel between points having potentially differing numbers of dimensions.  

\subsection{The problem}
To begin with, we can imagine trying to model the performance of neural networks with either one or two layers, with respect to the regualization parameters for each layer, $x_1$ and $x_2$.  If $y$ represents the performance of a one layer-net with regularization parameters $x_1$ and $x_2$, then the value $x_2$ doesn't matter, since there is no second layer to the network.

In this setting, we want a kernel on conditional spaces to have the following properties:

\begin{itemize}
\item If we are comparing two points with the same number of parameters, the value of any unused parameters shouldn't matter.  
$k(x_1, \textnormal{false}, x_2, x_1', \textnormal{false}, x_2') 
= k(x_1, \textnormal{false}, x_2'', x_1', \textnormal{false}, x_2''')
\forall x_2, x_2', x_2'', x_2'''$
\item The covariance between points using both parameters and points only using one should again only depend on their shared parameters.
$k(x_1, \textnormal{false}, x_2, x_1', \textnormal{true}, x_2') 
= k(x_1, \textnormal{false}, x_2'', x_1', \textnormal{true}, x_2''')
\forall x_2, x_2', x_2'', x_2'''$
\end{itemize}

\subsection{Cylindrical Embedding}

We can build a kernel with these properties by and embedding of points into a hiher-dimensional space than they began, and performing regression in that space.  Specifically, the embedding we use is:
%
%To emphasize that we're in the real case, we explicitly denote the pseudometric as $d\br_i$ and the (pseudo-)isometry from $(\sX, d_i)$ to $\reals^2,d_\text{E}$ 
%as $f\br_i$. For the definitions, recall that $\delta_i(\v{x})$ is true iff dimension $i$ is active given the instantiation of $i$'s ancestors in $\v{x}$.
%
%
\begin{eqnarray}
\nonumber{}f_i\br(\v{x}) & = & \left\{\begin{array}{ll}
[0,0]^\transpose & \textrm{ if } \delta_i(\v{x}) = \textrm{ false }\\
\nonumber{} \omega_i [\sin{\pi\rho_i\frac{x_i}{u_i-l_i}}, \cos{\pi\rho_i\frac{x_i}{u_i-l_i}}]^\transpose & \textrm{ otherwise.}\end{array}\right..
\end{eqnarray}
%
\begin{figure}
\input{figures/semicylinder}
\caption{A demonstration of the embedding giving rise to the pseduo-metric in 2 dimensions.  All points for which $\delta_i(x) =$ false are mapped onto a line varying only along $x_1$.  Points for which $\delta_i(x) =$ true are mapped to the surface of a semicylinder, depending on both $x_1$ and $x_2$.  This embedding gives a constant distance between pairs of points which have differing values of $\delta$ but the same values of $x_1$.
%  The parameter $\rho$ determines how much distance there is along the arc.
}
\label{fig:cylinder}
\end{figure}
%
Figure \ref{fig:cylinder} shows a visualization of the embedding of points $x$ into a higher-dimensional space with relative distances that only depend on active parameters.
%
This embedding gives rise to the Euclidian distances:
%
\begin{eqnarray}
\nonumber{}d\br_i(\v{x}, \v{x}') & = & \left\{\begin{array}{ll}
\nonumber{} 0 & \textrm{ if } \delta_i(\v{x}) = \delta_i(\v{x}') = \textrm{false}\\
\nonumber{} \omega_i & \textrm{ if } \delta_i(\v{x}) \neq \delta_i(\v{x}')\\
\nonumber{} \omega_i \sqrt{2} \sqrt{1 - \cos(\pi\rho_i \frac{x_i-x_i'}{u_i-l_i})} & \textrm{ if } \delta_i(\v{x}) = \delta_i(\v{x}') = \textrm{true}. \end{array}\right.
\end{eqnarray}

For kernels such as the exponentiated quadratic $k(x, x') = \sigma^2_f \exp \left( \nicefrac{-(x - x')^2}{2\ell^2} \right)$, or the Mat\'{e}rn, which only depend on the Euclidian distance, the covariance has the desired properties.






\section{Experiments}

\paragraph{Regression}   Bayesian optimization requires building a model of the function being optimized, and better models can be expected to lead to better outcomes.  However, because of the many interating components of BO, optimizer performance might not correspond directly to the quality of the model.  In this section, we isolate to what extent the conditional kernel is a better model than the alternatives.

\begin{table}[h!]
\caption{{\small
Normalized Mean Squared Error on Neural Network data
}}
\label{tbl:nn_nmse}
\input{tables/gpml-table.tex}
\end{table}

\paragraph{Models}
Separate Linear and Seperate {\sc gp} build a separate model for each neural net architecture, as in \cite{bergstra2011algorithms}.  The hierarchical {\sc gp} model combines all data together using the conditional kernel.
%
In the case where all data comes from the same architecture, the hierarchical {\sc gp} model makes slightly different assumptions than a standard {\sc gp}, because it places the data on a semi-circle.  To check whether this alternate assumption affects performance, we also include a Separate-Hierarchical {\sc gp} model.
%
We also compare against a simpler embedding method, or ``Poor Man's Embedding''.  In this procedure, we combine data from all models into a single model, replacing any unused parameters with a heuristic constant, set to $-1$ in these experiments.


\section{Conclusion}



\bibliography{hierarchicalkernel}
\bibliographystyle{unsrt}


\end{document}

